{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run this if you have some issues with Keras. It's meant for GPU memory allocations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.log_device_placement = True# to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Bidirectional, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are lyrics all belonging to one artist (Mitski) from Genius that I processed and cleaned. (If you want the script for that I can send it to, if you're ever looking to do something with lyrics!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>l_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be the Cowboy</td>\n",
       "      <td>A Horse Named Cold Air</td>\n",
       "      <td>a lake with no fish \\r\\n is the heart of a hor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Be the Cowboy</td>\n",
       "      <td>A Pearl</td>\n",
       "      <td>you are growing tired of me \\r\\n you love me s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Be the Cowboy</td>\n",
       "      <td>Blue Light</td>\n",
       "      <td>somebody kiss me i am going crazy \\r\\n i am wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Be the Cowboy</td>\n",
       "      <td>Come Into the Water</td>\n",
       "      <td>come into the water \\r\\n do you wanna be my ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Be the Cowboy</td>\n",
       "      <td>Geyser</td>\n",
       "      <td>you are my number one \\r\\n you are the one i w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           album                    song  \\\n",
       "0  Be the Cowboy  A Horse Named Cold Air   \n",
       "1  Be the Cowboy                 A Pearl   \n",
       "2  Be the Cowboy              Blue Light   \n",
       "3  Be the Cowboy     Come Into the Water   \n",
       "4  Be the Cowboy                  Geyser   \n",
       "\n",
       "                                               l_str  \n",
       "0  a lake with no fish \\r\\n is the heart of a hor...  \n",
       "1  you are growing tired of me \\r\\n you love me s...  \n",
       "2  somebody kiss me i am going crazy \\r\\n i am wa...  \n",
       "3  come into the water \\r\\n do you wanna be my ba...  \n",
       "4  you are my number one \\r\\n you are the one i w...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df = pd.read_csv('mitski_lyrics.csv')\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines all of the song lyrics into one large string. Adds \"END_OF_SONG\" and removes any extra whitespace via regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" END_OF_SONG \".join(lyrics_df['l_str'])\n",
    "# all_text = re.sub('\\\\r\\\\n','LINE_BREAK',all_text)\n",
    "all_text = re.sub('\\s{2,}', ' ', all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "splitted = all_text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_counts is a dictionary of the vocabulary present and a count of words. This could be used to filter out any rare words but I do not do that with this liimited dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "for word in splitted:\n",
    "    word_counts[word] = word_counts.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are dictionaries for taking a word between its string representation to is index representation and vice versa.\n",
    "\n",
    "Mainly used for OHE and for the class of 'y', not as necessary in Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(splitted)\n",
    "word_to_index = dict((c,i) for i,c in enumerate(words))\n",
    "index_to_word = dict((i,c) for i,c in enumerate(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goes through the text corpus with a rolling window of size 5 (x), as well as grabbbing the 6th word as the 'next word' (y).\n",
    "If it sees the end of a song, it skips adding that to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(splitted) - sequence_length):\n",
    "    sequence = splitted[i: i + sequence_length]\n",
    "    next_word = splitted[i+sequence_length]\n",
    "    if 'END_OF_SONG' in sequence or 'END_OF_SONG' == next_word:\n",
    "        continue\n",
    "    sequences.append(sequence)\n",
    "    next_words.append(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['i', 'am', 'the', 'same', 'as', 'all', 'those', 'men', 'writing', 'songs'], Y: of\n",
      "X: ['of', 'me', 'you', 'love', 'me', 'so', 'hard', 'and', 'i', 'still'], Y: ca\n"
     ]
    }
   ],
   "source": [
    "print('X: {}, Y: {}'.format(sequences[323], next_words[323]))\n",
    "print('X: {}, Y: {}'.format(sequences[35], next_words[35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7246"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates x -> zero array of rows * sequence_length * vocab_size\n",
    "\n",
    "Creates y -> array of next_words (indexed)\n",
    "\n",
    "One hot encodes entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_convert(word):\n",
    "    return word_to_index[word]\n",
    "\n",
    "x = np.zeros((len(sequences), sequence_length, len(words)))\n",
    "y = np.fromiter(map(word_convert, next_words), dtype = np.int)\n",
    "\n",
    "for i in range(0,len(sequences)):\n",
    "    sequence_indexed = list(map(word_convert, sequences[i]))\n",
    "    for j in range(0,sequence_length):\n",
    "        x[i, j, sequence_indexed[j]] = 1\n",
    "\n",
    "y_ohe = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B/glove.6B.100d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts x into embedding representation. \n",
    "\n",
    "x_emb is a matrix of n * sequence_length * embedding_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_convert(word):\n",
    "    return embeddings_index[word]\n",
    "\n",
    "x_emb = np.zeros((len(sequences), sequence_length, len(embeddings_index['the'])))\n",
    "\n",
    "for i in range(0,len(sequences)):\n",
    "    sequence_embedded = list(map(embedding_convert, sequences[i]))\n",
    "    for j in range(0,sequence_length):\n",
    "        x_emb[i, :] = sequence_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7246,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7246, 10, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7246, 997)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoded Version of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 64)                263680    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 997)               64805     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 997)               0         \n",
      "=================================================================\n",
      "Total params: 328,485\n",
      "Trainable params: 328,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "7246/7246 [==============================] - 80s 11ms/step - loss: 6.3022 - acc: 0.0620\n",
      "Epoch 2/100\n",
      "7246/7246 [==============================] - 4s 615us/step - loss: 5.5523 - acc: 0.0708\n",
      "Epoch 3/100\n",
      "7246/7246 [==============================] - 4s 612us/step - loss: 5.4806 - acc: 0.0708\n",
      "Epoch 4/100\n",
      "7246/7246 [==============================] - 4s 602us/step - loss: 5.4006 - acc: 0.0686\n",
      "Epoch 5/100\n",
      "7246/7246 [==============================] - 4s 594us/step - loss: 5.3236 - acc: 0.0667\n",
      "Epoch 6/100\n",
      "7246/7246 [==============================] - 4s 596us/step - loss: 5.2416 - acc: 0.0690\n",
      "Epoch 7/100\n",
      "7246/7246 [==============================] - 4s 592us/step - loss: 5.1734 - acc: 0.0719\n",
      "Epoch 8/100\n",
      "7246/7246 [==============================] - 4s 585us/step - loss: 5.1225 - acc: 0.0712\n",
      "Epoch 9/100\n",
      "7246/7246 [==============================] - 4s 590us/step - loss: 5.1036 - acc: 0.0712\n",
      "Epoch 10/100\n",
      "7246/7246 [==============================] - 4s 590us/step - loss: 5.0545 - acc: 0.0747\n",
      "Epoch 11/100\n",
      "7246/7246 [==============================] - 4s 590us/step - loss: 4.9955 - acc: 0.0795\n",
      "Epoch 12/100\n",
      "7246/7246 [==============================] - 4s 580us/step - loss: 4.9485 - acc: 0.0828\n",
      "Epoch 13/100\n",
      "7246/7246 [==============================] - 4s 606us/step - loss: 4.9040 - acc: 0.0883\n",
      "Epoch 14/100\n",
      "7246/7246 [==============================] - 5s 670us/step - loss: 4.8539 - acc: 0.0918\n",
      "Epoch 15/100\n",
      "7246/7246 [==============================] - 6s 856us/step - loss: 4.8144 - acc: 0.0992\n",
      "Epoch 16/100\n",
      "7246/7246 [==============================] - 5s 705us/step - loss: 4.7684 - acc: 0.1076\n",
      "Epoch 17/100\n",
      "7246/7246 [==============================] - 5s 634us/step - loss: 4.7218 - acc: 0.1144\n",
      "Epoch 18/100\n",
      "7246/7246 [==============================] - 5s 632us/step - loss: 4.6604 - acc: 0.1201\n",
      "Epoch 19/100\n",
      "7246/7246 [==============================] - 5s 658us/step - loss: 4.5796 - acc: 0.1274\n",
      "Epoch 20/100\n",
      "7246/7246 [==============================] - 4s 619us/step - loss: 4.5117 - acc: 0.1457\n",
      "Epoch 21/100\n",
      "7246/7246 [==============================] - 5s 622us/step - loss: 4.4398 - acc: 0.1580\n",
      "Epoch 22/100\n",
      "7246/7246 [==============================] - 4s 605us/step - loss: 4.3634 - acc: 0.1706\n",
      "Epoch 23/100\n",
      "7246/7246 [==============================] - 4s 621us/step - loss: 4.3004 - acc: 0.1722\n",
      "Epoch 24/100\n",
      "7246/7246 [==============================] - 4s 615us/step - loss: 4.2328 - acc: 0.1884\n",
      "Epoch 25/100\n",
      "7246/7246 [==============================] - 5s 624us/step - loss: 4.1719 - acc: 0.1914\n",
      "Epoch 26/100\n",
      "7246/7246 [==============================] - 5s 646us/step - loss: 4.1007 - acc: 0.2007\n",
      "Epoch 27/100\n",
      "7246/7246 [==============================] - 4s 618us/step - loss: 4.0132 - acc: 0.2113\n",
      "Epoch 28/100\n",
      "7246/7246 [==============================] - 4s 615us/step - loss: 3.9373 - acc: 0.2197\n",
      "Epoch 29/100\n",
      "7246/7246 [==============================] - 5s 640us/step - loss: 3.8721 - acc: 0.2276\n",
      "Epoch 30/100\n",
      "7246/7246 [==============================] - 5s 645us/step - loss: 3.7997 - acc: 0.2331\n",
      "Epoch 31/100\n",
      "7246/7246 [==============================] - 5s 632us/step - loss: 3.7482 - acc: 0.2440\n",
      "Epoch 32/100\n",
      "7246/7246 [==============================] - 4s 602us/step - loss: 3.6894 - acc: 0.2509\n",
      "Epoch 33/100\n",
      "7246/7246 [==============================] - 4s 603us/step - loss: 3.6401 - acc: 0.2575\n",
      "Epoch 34/100\n",
      "7246/7246 [==============================] - 4s 618us/step - loss: 3.5665 - acc: 0.2687\n",
      "Epoch 35/100\n",
      "7246/7246 [==============================] - 4s 598us/step - loss: 3.4853 - acc: 0.2779\n",
      "Epoch 36/100\n",
      "7246/7246 [==============================] - 4s 611us/step - loss: 3.4119 - acc: 0.2898\n",
      "Epoch 37/100\n",
      "7246/7246 [==============================] - 4s 603us/step - loss: 3.3409 - acc: 0.2988\n",
      "Epoch 38/100\n",
      "7246/7246 [==============================] - 4s 605us/step - loss: 3.2644 - acc: 0.3091\n",
      "Epoch 39/100\n",
      "7246/7246 [==============================] - 5s 636us/step - loss: 3.1987 - acc: 0.3205 2s \n",
      "Epoch 40/100\n",
      "7246/7246 [==============================] - 5s 636us/step - loss: 3.1411 - acc: 0.3308\n",
      "Epoch 41/100\n",
      "7246/7246 [==============================] - 5s 632us/step - loss: 3.0884 - acc: 0.3403\n",
      "Epoch 42/100\n",
      "7246/7246 [==============================] - 5s 623us/step - loss: 3.0293 - acc: 0.3494\n",
      "Epoch 43/100\n",
      "7246/7246 [==============================] - 5s 623us/step - loss: 2.9709 - acc: 0.3552\n",
      "Epoch 44/100\n",
      "7246/7246 [==============================] - 5s 622us/step - loss: 2.8971 - acc: 0.3692\n",
      "Epoch 45/100\n",
      "7246/7246 [==============================] - 5s 626us/step - loss: 2.8242 - acc: 0.3839\n",
      "Epoch 46/100\n",
      "7246/7246 [==============================] - ETA: 0s - loss: 2.7663 - acc: 0.394 - 5s 624us/step - loss: 2.7636 - acc: 0.3948\n",
      "Epoch 47/100\n",
      "7246/7246 [==============================] - 4s 617us/step - loss: 2.6844 - acc: 0.4100\n",
      "Epoch 48/100\n",
      "7246/7246 [==============================] - 4s 606us/step - loss: 2.6150 - acc: 0.4237\n",
      "Epoch 49/100\n",
      "7246/7246 [==============================] - 4s 614us/step - loss: 2.5490 - acc: 0.4354\n",
      "Epoch 50/100\n",
      "7246/7246 [==============================] - 4s 600us/step - loss: 2.4856 - acc: 0.4511\n",
      "Epoch 51/100\n",
      "7246/7246 [==============================] - 4s 596us/step - loss: 2.4332 - acc: 0.4643\n",
      "Epoch 52/100\n",
      "7246/7246 [==============================] - 4s 590us/step - loss: 2.3781 - acc: 0.4752\n",
      "Epoch 53/100\n",
      "7246/7246 [==============================] - 4s 599us/step - loss: 2.3358 - acc: 0.4818\n",
      "Epoch 54/100\n",
      "7246/7246 [==============================] - 4s 597us/step - loss: 2.2840 - acc: 0.5001\n",
      "Epoch 55/100\n",
      "7246/7246 [==============================] - 4s 595us/step - loss: 2.2063 - acc: 0.5132\n",
      "Epoch 56/100\n",
      "7246/7246 [==============================] - 4s 604us/step - loss: 2.1327 - acc: 0.5305\n",
      "Epoch 57/100\n",
      "7246/7246 [==============================] - 4s 600us/step - loss: 2.0645 - acc: 0.5457\n",
      "Epoch 58/100\n",
      "7246/7246 [==============================] - 4s 621us/step - loss: 1.9949 - acc: 0.5650\n",
      "Epoch 59/100\n",
      "7246/7246 [==============================] - 5s 665us/step - loss: 1.9366 - acc: 0.5821\n",
      "Epoch 60/100\n",
      "7246/7246 [==============================] - 5s 650us/step - loss: 1.8890 - acc: 0.5882\n",
      "Epoch 61/100\n",
      "7246/7246 [==============================] - 5s 657us/step - loss: 1.8441 - acc: 0.5995 1s - loss: 1.8626\n",
      "Epoch 62/100\n",
      "7246/7246 [==============================] - 5s 662us/step - loss: 1.8050 - acc: 0.6041\n",
      "Epoch 63/100\n",
      "7246/7246 [==============================] - 5s 653us/step - loss: 1.7540 - acc: 0.6143\n",
      "Epoch 64/100\n",
      "7246/7246 [==============================] - 5s 644us/step - loss: 1.7023 - acc: 0.6306\n",
      "Epoch 65/100\n",
      "7246/7246 [==============================] - 5s 658us/step - loss: 1.6472 - acc: 0.6438\n",
      "Epoch 66/100\n",
      "7246/7246 [==============================] - 5s 630us/step - loss: 1.6010 - acc: 0.6626\n",
      "Epoch 67/100\n",
      "7246/7246 [==============================] - 4s 605us/step - loss: 1.5653 - acc: 0.6692\n",
      "Epoch 68/100\n",
      "7246/7246 [==============================] - 4s 605us/step - loss: 1.5144 - acc: 0.6811\n",
      "Epoch 69/100\n",
      "7246/7246 [==============================] - 4s 611us/step - loss: 1.4657 - acc: 0.6943\n",
      "Epoch 70/100\n",
      "7246/7246 [==============================] - 4s 612us/step - loss: 1.4173 - acc: 0.7091\n",
      "Epoch 71/100\n",
      "7246/7246 [==============================] - 4s 607us/step - loss: 1.3839 - acc: 0.7153\n",
      "Epoch 72/100\n",
      "7246/7246 [==============================] - 4s 605us/step - loss: 1.3304 - acc: 0.7285\n",
      "Epoch 73/100\n",
      "7246/7246 [==============================] - 4s 608us/step - loss: 1.2943 - acc: 0.7379\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7246/7246 [==============================] - 4s 620us/step - loss: 1.2496 - acc: 0.7476\n",
      "Epoch 75/100\n",
      "7246/7246 [==============================] - 5s 623us/step - loss: 1.2096 - acc: 0.7559\n",
      "Epoch 76/100\n",
      "7246/7246 [==============================] - 5s 627us/step - loss: 1.1761 - acc: 0.7630\n",
      "Epoch 77/100\n",
      "7246/7246 [==============================] - 5s 630us/step - loss: 1.1374 - acc: 0.7735\n",
      "Epoch 78/100\n",
      "7246/7246 [==============================] - 5s 626us/step - loss: 1.0885 - acc: 0.7894\n",
      "Epoch 79/100\n",
      "7246/7246 [==============================] - 5s 622us/step - loss: 1.0533 - acc: 0.7974\n",
      "Epoch 80/100\n",
      "7246/7246 [==============================] - 5s 635us/step - loss: 1.0234 - acc: 0.8054\n",
      "Epoch 81/100\n",
      "7246/7246 [==============================] - 5s 646us/step - loss: 0.9898 - acc: 0.8098\n",
      "Epoch 82/100\n",
      "7246/7246 [==============================] - 5s 630us/step - loss: 0.9526 - acc: 0.8202\n",
      "Epoch 83/100\n",
      "7246/7246 [==============================] - 5s 627us/step - loss: 0.9211 - acc: 0.8274\n",
      "Epoch 84/100\n",
      "7246/7246 [==============================] - 5s 637us/step - loss: 0.8893 - acc: 0.8344\n",
      "Epoch 85/100\n",
      "7246/7246 [==============================] - 5s 626us/step - loss: 0.8536 - acc: 0.8452\n",
      "Epoch 86/100\n",
      "7246/7246 [==============================] - 5s 723us/step - loss: 0.8265 - acc: 0.8503\n",
      "Epoch 87/100\n",
      "7246/7246 [==============================] - 5s 636us/step - loss: 0.8048 - acc: 0.8565\n",
      "Epoch 88/100\n",
      "7246/7246 [==============================] - 4s 604us/step - loss: 0.7878 - acc: 0.8576\n",
      "Epoch 89/100\n",
      "7246/7246 [==============================] - 5s 623us/step - loss: 0.7765 - acc: 0.8570\n",
      "Epoch 90/100\n",
      "7246/7246 [==============================] - 4s 598us/step - loss: 0.7480 - acc: 0.8627\n",
      "Epoch 91/100\n",
      "7246/7246 [==============================] - 4s 602us/step - loss: 0.7179 - acc: 0.8717\n",
      "Epoch 92/100\n",
      "7246/7246 [==============================] - 4s 591us/step - loss: 0.6837 - acc: 0.8802\n",
      "Epoch 93/100\n",
      "7246/7246 [==============================] - 4s 587us/step - loss: 0.6573 - acc: 0.8853\n",
      "Epoch 94/100\n",
      "7246/7246 [==============================] - 4s 590us/step - loss: 0.6354 - acc: 0.8955\n",
      "Epoch 95/100\n",
      "7246/7246 [==============================] - 4s 588us/step - loss: 0.6206 - acc: 0.8990\n",
      "Epoch 96/100\n",
      "7246/7246 [==============================] - 4s 602us/step - loss: 0.6085 - acc: 0.9001\n",
      "Epoch 97/100\n",
      "7246/7246 [==============================] - 4s 589us/step - loss: 0.6018 - acc: 0.8958\n",
      "Epoch 98/100\n",
      "7246/7246 [==============================] - 4s 577us/step - loss: 0.5918 - acc: 0.8997\n",
      "Epoch 99/100\n",
      "7246/7246 [==============================] - 4s 582us/step - loss: 0.5742 - acc: 0.9048\n",
      "Epoch 100/100\n",
      "7246/7246 [==============================] - 4s 605us/step - loss: 0.5446 - acc: 0.9117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e089db8470>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ohe = Sequential()\n",
    "model_ohe.add(Bidirectional(LSTM(32), input_shape=(sequence_length, len(words))))\n",
    "model_ohe.add(Dense(len(words)))\n",
    "model_ohe.add(Activation('softmax'))\n",
    "\n",
    "print(model_ohe.summary())\n",
    "\n",
    "model_ohe.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_ohe.fit(x, y_ohe, batch_size = 64, epochs = 100, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Version of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_4 (Bidirection (None, 64)                34048     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 997)               64805     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 997)               0         \n",
      "=================================================================\n",
      "Total params: 98,853\n",
      "Trainable params: 98,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "7246/7246 [==============================] - 108s 15ms/step - loss: 6.0452 - acc: 0.0668\n",
      "Epoch 2/100\n",
      "7246/7246 [==============================] - 4s 547us/step - loss: 5.5273 - acc: 0.0749\n",
      "Epoch 3/100\n",
      "7246/7246 [==============================] - 4s 527us/step - loss: 5.4549 - acc: 0.0737\n",
      "Epoch 4/100\n",
      "7246/7246 [==============================] - 4s 508us/step - loss: 5.3713 - acc: 0.0760\n",
      "Epoch 5/100\n",
      "7246/7246 [==============================] - 4s 511us/step - loss: 5.2832 - acc: 0.0778\n",
      "Epoch 6/100\n",
      "7246/7246 [==============================] - 4s 519us/step - loss: 5.1911 - acc: 0.0858\n",
      "Epoch 7/100\n",
      "7246/7246 [==============================] - 4s 520us/step - loss: 5.0995 - acc: 0.0897\n",
      "Epoch 8/100\n",
      "7246/7246 [==============================] - 4s 511us/step - loss: 5.0058 - acc: 0.0988\n",
      "Epoch 9/100\n",
      "7246/7246 [==============================] - 4s 508us/step - loss: 4.9144 - acc: 0.1060\n",
      "Epoch 10/100\n",
      "7246/7246 [==============================] - 4s 506us/step - loss: 4.8231 - acc: 0.1139\n",
      "Epoch 11/100\n",
      "7246/7246 [==============================] - 4s 514us/step - loss: 4.7343 - acc: 0.1235\n",
      "Epoch 12/100\n",
      "7246/7246 [==============================] - 4s 528us/step - loss: 4.6416 - acc: 0.1341\n",
      "Epoch 13/100\n",
      "7246/7246 [==============================] - 4s 522us/step - loss: 4.5460 - acc: 0.1431 1s - loss: 4.56\n",
      "Epoch 14/100\n",
      "7246/7246 [==============================] - 4s 555us/step - loss: 4.4544 - acc: 0.1511\n",
      "Epoch 15/100\n",
      "7246/7246 [==============================] - 5s 640us/step - loss: 4.3851 - acc: 0.1601\n",
      "Epoch 16/100\n",
      "7246/7246 [==============================] - 4s 584us/step - loss: 4.2919 - acc: 0.1645\n",
      "Epoch 17/100\n",
      "7246/7246 [==============================] - 4s 555us/step - loss: 4.2062 - acc: 0.1755\n",
      "Epoch 18/100\n",
      "7246/7246 [==============================] - 4s 561us/step - loss: 4.1318 - acc: 0.1827\n",
      "Epoch 19/100\n",
      "7246/7246 [==============================] - 4s 527us/step - loss: 4.0533 - acc: 0.1866\n",
      "Epoch 20/100\n",
      "7246/7246 [==============================] - 4s 521us/step - loss: 3.9857 - acc: 0.1980 1s - loss\n",
      "Epoch 21/100\n",
      "7246/7246 [==============================] - 4s 528us/step - loss: 3.9174 - acc: 0.2083\n",
      "Epoch 22/100\n",
      "7246/7246 [==============================] - 4s 548us/step - loss: 3.8416 - acc: 0.2156\n",
      "Epoch 23/100\n",
      "7246/7246 [==============================] - 4s 530us/step - loss: 3.7668 - acc: 0.2276\n",
      "Epoch 24/100\n",
      "7246/7246 [==============================] - 4s 514us/step - loss: 3.6879 - acc: 0.2332 0s - loss: 3.7013 - a\n",
      "Epoch 25/100\n",
      "7246/7246 [==============================] - 4s 518us/step - loss: 3.6122 - acc: 0.2407\n",
      "Epoch 26/100\n",
      "7246/7246 [==============================] - 4s 518us/step - loss: 3.5488 - acc: 0.2523\n",
      "Epoch 27/100\n",
      "7246/7246 [==============================] - 4s 550us/step - loss: 3.4874 - acc: 0.2607\n",
      "Epoch 28/100\n",
      "7246/7246 [==============================] - 4s 532us/step - loss: 3.4343 - acc: 0.2676\n",
      "Epoch 29/100\n",
      "7246/7246 [==============================] - 4s 544us/step - loss: 3.3785 - acc: 0.2788\n",
      "Epoch 30/100\n",
      "7246/7246 [==============================] - 4s 534us/step - loss: 3.3139 - acc: 0.2866\n",
      "Epoch 31/100\n",
      "7246/7246 [==============================] - 4s 517us/step - loss: 3.2602 - acc: 0.2999 0s - loss: 3.2750 - a\n",
      "Epoch 32/100\n",
      "7246/7246 [==============================] - 4s 510us/step - loss: 3.1899 - acc: 0.3104\n",
      "Epoch 33/100\n",
      "7246/7246 [==============================] - 4s 509us/step - loss: 3.1162 - acc: 0.3264\n",
      "Epoch 34/100\n",
      "7246/7246 [==============================] - 4s 540us/step - loss: 3.0520 - acc: 0.3358\n",
      "Epoch 35/100\n",
      "7246/7246 [==============================] - 4s 524us/step - loss: 2.9870 - acc: 0.3496\n",
      "Epoch 36/100\n",
      "7246/7246 [==============================] - 4s 535us/step - loss: 2.9207 - acc: 0.3601\n",
      "Epoch 37/100\n",
      "7246/7246 [==============================] - 4s 535us/step - loss: 2.8654 - acc: 0.3718\n",
      "Epoch 38/100\n",
      "7246/7246 [==============================] - 4s 564us/step - loss: 2.8045 - acc: 0.3827\n",
      "Epoch 39/100\n",
      "7246/7246 [==============================] - 4s 546us/step - loss: 2.7434 - acc: 0.3973\n",
      "Epoch 40/100\n",
      "7246/7246 [==============================] - 4s 546us/step - loss: 2.6877 - acc: 0.4055\n",
      "Epoch 41/100\n",
      "7246/7246 [==============================] - 4s 519us/step - loss: 2.6307 - acc: 0.4200\n",
      "Epoch 42/100\n",
      "7246/7246 [==============================] - 4s 590us/step - loss: 2.5735 - acc: 0.4362 1s - loss: 2.5856 - acc: - ETA: 0s - loss: 2.5788 - ac\n",
      "Epoch 43/100\n",
      "7246/7246 [==============================] - 4s 553us/step - loss: 2.5245 - acc: 0.4473\n",
      "Epoch 44/100\n",
      "7246/7246 [==============================] - 4s 551us/step - loss: 2.4776 - acc: 0.4536\n",
      "Epoch 45/100\n",
      "7246/7246 [==============================] - 4s 580us/step - loss: 2.4354 - acc: 0.4629\n",
      "Epoch 46/100\n",
      "7246/7246 [==============================] - 4s 542us/step - loss: 2.3997 - acc: 0.4705\n",
      "Epoch 47/100\n",
      "7246/7246 [==============================] - 4s 534us/step - loss: 2.3460 - acc: 0.4804\n",
      "Epoch 48/100\n",
      "7246/7246 [==============================] - 4s 522us/step - loss: 2.2969 - acc: 0.4930\n",
      "Epoch 49/100\n",
      "7246/7246 [==============================] - 4s 539us/step - loss: 2.2543 - acc: 0.5014\n",
      "Epoch 50/100\n",
      "7246/7246 [==============================] - 4s 550us/step - loss: 2.2165 - acc: 0.5079\n",
      "Epoch 51/100\n",
      "7246/7246 [==============================] - 4s 569us/step - loss: 2.1678 - acc: 0.5200 0s - loss: 2.1768 - ac\n",
      "Epoch 52/100\n",
      "7246/7246 [==============================] - 4s 571us/step - loss: 2.1244 - acc: 0.5308\n",
      "Epoch 53/100\n",
      "7246/7246 [==============================] - 4s 566us/step - loss: 2.0802 - acc: 0.5391\n",
      "Epoch 54/100\n",
      "7246/7246 [==============================] - 4s 587us/step - loss: 2.0416 - acc: 0.5469\n",
      "Epoch 55/100\n",
      "7246/7246 [==============================] - 4s 592us/step - loss: 2.0047 - acc: 0.5571\n",
      "Epoch 56/100\n",
      "7246/7246 [==============================] - 4s 587us/step - loss: 1.9660 - acc: 0.5683\n",
      "Epoch 57/100\n",
      "7246/7246 [==============================] - 4s 613us/step - loss: 1.9353 - acc: 0.5729\n",
      "Epoch 58/100\n",
      "7246/7246 [==============================] - 4s 547us/step - loss: 1.9051 - acc: 0.5794\n",
      "Epoch 59/100\n",
      "7246/7246 [==============================] - 4s 523us/step - loss: 1.8706 - acc: 0.5879\n",
      "Epoch 60/100\n",
      "7246/7246 [==============================] - 4s 506us/step - loss: 1.8411 - acc: 0.5970\n",
      "Epoch 61/100\n",
      "7246/7246 [==============================] - 4s 521us/step - loss: 1.8246 - acc: 0.5983\n",
      "Epoch 62/100\n",
      "7246/7246 [==============================] - 4s 523us/step - loss: 1.7945 - acc: 0.6052\n",
      "Epoch 63/100\n",
      "7246/7246 [==============================] - 4s 559us/step - loss: 1.7678 - acc: 0.6108\n",
      "Epoch 64/100\n",
      "7246/7246 [==============================] - 4s 547us/step - loss: 1.7488 - acc: 0.6133\n",
      "Epoch 65/100\n",
      "7246/7246 [==============================] - 4s 551us/step - loss: 1.7228 - acc: 0.6184 1s - loss\n",
      "Epoch 66/100\n",
      "7246/7246 [==============================] - 4s 553us/step - loss: 1.7009 - acc: 0.6221\n",
      "Epoch 67/100\n",
      "7246/7246 [==============================] - 4s 551us/step - loss: 1.6679 - acc: 0.6281\n",
      "Epoch 68/100\n",
      "7246/7246 [==============================] - 4s 541us/step - loss: 1.6388 - acc: 0.6350\n",
      "Epoch 69/100\n",
      "7246/7246 [==============================] - 4s 539us/step - loss: 1.6054 - acc: 0.6417\n",
      "Epoch 70/100\n",
      "7246/7246 [==============================] - 4s 506us/step - loss: 1.5805 - acc: 0.6460\n",
      "Epoch 71/100\n",
      "7246/7246 [==============================] - 4s 504us/step - loss: 1.5496 - acc: 0.6496\n",
      "Epoch 72/100\n",
      "7246/7246 [==============================] - 4s 506us/step - loss: 1.5257 - acc: 0.6598\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7246/7246 [==============================] - 4s 531us/step - loss: 1.4920 - acc: 0.6688\n",
      "Epoch 74/100\n",
      "7246/7246 [==============================] - 4s 526us/step - loss: 1.4695 - acc: 0.6750\n",
      "Epoch 75/100\n",
      "7246/7246 [==============================] - 4s 539us/step - loss: 1.4397 - acc: 0.6847\n",
      "Epoch 76/100\n",
      "7246/7246 [==============================] - 4s 521us/step - loss: 1.4196 - acc: 0.6880\n",
      "Epoch 77/100\n",
      "7246/7246 [==============================] - 4s 542us/step - loss: 1.4044 - acc: 0.6896\n",
      "Epoch 78/100\n",
      "7246/7246 [==============================] - 4s 527us/step - loss: 1.3645 - acc: 0.7018\n",
      "Epoch 79/100\n",
      "7246/7246 [==============================] - 4s 536us/step - loss: 1.3343 - acc: 0.7096\n",
      "Epoch 80/100\n",
      "7246/7246 [==============================] - 4s 532us/step - loss: 1.3056 - acc: 0.7161\n",
      "Epoch 81/100\n",
      "7246/7246 [==============================] - 4s 537us/step - loss: 1.2790 - acc: 0.7201\n",
      "Epoch 82/100\n",
      "7246/7246 [==============================] - 4s 545us/step - loss: 1.2566 - acc: 0.7261\n",
      "Epoch 83/100\n",
      "7246/7246 [==============================] - 4s 533us/step - loss: 1.2341 - acc: 0.7323\n",
      "Epoch 84/100\n",
      "7246/7246 [==============================] - 4s 546us/step - loss: 1.2150 - acc: 0.7359\n",
      "Epoch 85/100\n",
      "7246/7246 [==============================] - 4s 569us/step - loss: 1.1980 - acc: 0.7408\n",
      "Epoch 86/100\n",
      "7246/7246 [==============================] - 4s 530us/step - loss: 1.1697 - acc: 0.7459\n",
      "Epoch 87/100\n",
      "7246/7246 [==============================] - 4s 516us/step - loss: 1.1475 - acc: 0.7550\n",
      "Epoch 88/100\n",
      "7246/7246 [==============================] - 4s 544us/step - loss: 1.1287 - acc: 0.7597\n",
      "Epoch 89/100\n",
      "7246/7246 [==============================] - 4s 515us/step - loss: 1.1141 - acc: 0.7625\n",
      "Epoch 90/100\n",
      "7246/7246 [==============================] - 4s 503us/step - loss: 1.0987 - acc: 0.7672\n",
      "Epoch 91/100\n",
      "7246/7246 [==============================] - 4s 515us/step - loss: 1.0881 - acc: 0.7695\n",
      "Epoch 92/100\n",
      "7246/7246 [==============================] - 4s 502us/step - loss: 1.0662 - acc: 0.7763 0s - loss: 1.0738 - a\n",
      "Epoch 93/100\n",
      "7246/7246 [==============================] - 4s 546us/step - loss: 1.0503 - acc: 0.7770\n",
      "Epoch 94/100\n",
      "7246/7246 [==============================] - 4s 508us/step - loss: 1.0293 - acc: 0.7843\n",
      "Epoch 95/100\n",
      "7246/7246 [==============================] - 4s 504us/step - loss: 1.0107 - acc: 0.7858\n",
      "Epoch 96/100\n",
      "7246/7246 [==============================] - 4s 525us/step - loss: 0.9912 - acc: 0.7888\n",
      "Epoch 97/100\n",
      "7246/7246 [==============================] - 4s 512us/step - loss: 0.9720 - acc: 0.7952\n",
      "Epoch 98/100\n",
      "7246/7246 [==============================] - 4s 513us/step - loss: 0.9531 - acc: 0.7989 0s - loss: 0.9547 - acc: 0.79\n",
      "Epoch 99/100\n",
      "7246/7246 [==============================] - 4s 510us/step - loss: 0.9378 - acc: 0.8033\n",
      "Epoch 100/100\n",
      "7246/7246 [==============================] - 4s 528us/step - loss: 0.9188 - acc: 0.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e08f28bba8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb = Sequential()\n",
    "model_emb.add(Bidirectional(LSTM(32), input_shape=(sequence_length, len(embeddings_index['the']))))\n",
    "model_emb.add(Dense(997))\n",
    "model_emb.add(Activation('softmax'))\n",
    "\n",
    "print(model_emb.summary())\n",
    "\n",
    "model_emb.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_emb.fit(x_emb, y_ohe, batch_size = 64, epochs = 100, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with the LSTM\n",
    "\n",
    "Insert some words into 'seed_sentences', hit run. Can change the 'effect_weight' (second paramater) in the 'choose_next_word()' function to add more or less randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This helps randomify the output during predictions so it is not always picking the top-word. \n",
    "#I found this here: https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb\n",
    "\n",
    "def choose_next_word(preds, effect_weight=1.0):\n",
    "    # preds = prediction array for possible vocabulary of next word\n",
    "    # effect_weight = weight to add some noise in order to change how \"random\" the next word predictions will be\n",
    "    # closer to 0 means less random (more likely to use stop words)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / effect_weight\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds) # Re-normalize probability similar to softmax\n",
    "    probas = np.random.multinomial(1, preds, 1) # List of random numbers given the preds p-vals\n",
    "    return np.argmax(probas) # Returns indice of the highest random number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a there is a man on the moon i will make every years just finally sleep other waiting for all i have to scream your scream how i could stand a outside and do i have been hungry better neat only down owns out looking out i can finally stay big and two twenty morning world the night i just need the quiet of rotting i told you\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "words_number = 60 # number of words to generate\n",
    "seed_sentences = \"there is a man on the moon\" #seed sentence to start the generating.\n",
    "\n",
    "#initiate sentences\n",
    "generated = ''\n",
    "sentence = []\n",
    "\n",
    "#we shate the seed accordingly to the neural netwrok needs:\n",
    "for i in range (sequence_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[sequence_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "\n",
    "#the, we generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    new_line = np.zeros((1, sequence_length, vocab_size))\n",
    "    \n",
    "    for t, word in enumerate(sentence):\n",
    "        new_line[0, t, word_to_index[word]] = 1\n",
    "     \n",
    "        \n",
    "    #calculate next word\n",
    "    preds = model_ohe.predict(new_line, verbose=0)[0]\n",
    "    next_index = choose_next_word(preds, 0.9)\n",
    "    next_word = index_to_word[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "#print the whole text\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a the man on the moon all why we work i am i am gonna waiting for a way i am waiting someone in not i may never down in my head all you will come him how my hand is losing more and i think all it is a of all empty of my life but i am doing even secret of something i just what you if i have been down i makes not just i am looking i i am chasing to go my body by you if i love more more more when i do not know at the white he first met your mother would you have you to watch me but i could see that i do not you a hundred times way inside me so and it is a little man it i do not be but i am done to silence the city i cannot see your fate tonight i just need than you wanna that you know that you want to you if you come at you are take me and i know you darling we i need tell you that you choose me baby you not come to me what my body wants me there is\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "words_number = 200 # number of words to generate\n",
    "seed_sentences = \"the man on the moon\" #seed sentence to start the generating.\n",
    "\n",
    "#initiate sentences\n",
    "generated = ''\n",
    "sentence = []\n",
    "\n",
    "#we shate the seed accordingly to the neural netwrok needs:\n",
    "for i in range (sequence_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[sequence_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "\n",
    "#the, we generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    new_line = np.zeros((1, sequence_length, len(embeddings_index['the'])))\n",
    "    \n",
    "    for t, word in enumerate(sentence):\n",
    "        new_line[0, t] = embedding_convert(word)\n",
    "     \n",
    "        \n",
    "    #calculate next word\n",
    "    preds = model_emb.predict(new_line, verbose=0)[0]\n",
    "    next_index = choose_next_word(preds,1)\n",
    "    next_word = index_to_word[next_index]\n",
    "    \n",
    "    while next_word == '':\n",
    "        preds = model_emb.predict(new_line, verbose=0)[0]\n",
    "        next_index = choose_next_word(preds, 0.75)\n",
    "        next_word = index_to_word[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "#print the whole text\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OHE: happy as i can be brave just here just out i love on it up what i live the violin him the ground it save i love my prime anything i will afraid talk talk go lined come love i fading of waiting not want\n",
    "\n",
    "Embedding: happy as i can be tried would leave my body i can come shown it secret waiting in it is funny how it take him me window under all of rushing roof to the sunlight as you all up out out need me lying chasing\n",
    "\n",
    "this works really great actually waited cookies warm summer night capture i love it looking to me what i can watch i bet move as you figure me what you pinky promise somebody your tonight fireworks badly my breath and smoke maybe does me baby\n",
    "\n",
    "\n",
    "the man on the moon i am liquid smooth come touch me too i will be watching i want i have you are the house and i am how to you am meant anything please me tell me no tell me no tell me no tell me tell me no tell me no tell me no tell me want me no tell me what you know tell me please give remember me but you are going watching to kiss look at your violin i will hear back the light cry cry it so cry cry cry cry cry cry cry cry cry me cry cry cry cry cry cry cry cry cry cry cry cry cry you cry cry cry cry cry cry cry cry cry me cry cry cry cry cry cry cry cry cry me cry cry cry cry cry oh call cry cry cry cry cry cry cry cry cry cry cry cry cry cry it cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry cry nobody they save me me with me what you take you sorry what you want it and you kiss your body is\n",
    "\n",
    "OHE: happy as i can be brave just here just out i love on it up what i live the violin him the ground it save i love my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"nobody wants to love and me i don't my there's once you me go i die and your want home and you it's i all think with scared know it whole you so me me and and me would me i i a you too starting you when i as you you it you through you and if you the alone i i i would you work hold away and me of you the i all you i i all hands your so i the what to slow i and to be me i'm you're me i all yet i my up you it of the from me in as my my but doo from danced take in you're my the me i table a\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
